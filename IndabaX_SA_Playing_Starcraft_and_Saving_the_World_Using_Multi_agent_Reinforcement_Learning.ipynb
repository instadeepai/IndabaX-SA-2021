{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "IndabaX_SA_Playing Starcraft and Saving the World Using Multi-agent Reinforcement Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8qY77_tnR2fo",
        "dTODoG38ez4R",
        "ohA5m0REjhu-",
        "lcZKJhnyk45C",
        "qBWiibHIleQk",
        "l8XqA9M2iyK_",
        "5i3tj4h-lTm4",
        "JfI2fNFeltBm",
        "XHf3jDe3ySk7",
        "SHygoBPW-3KV",
        "5TwjhQ0K4_yd",
        "XKne7VqnOI_O",
        "PmkPnQalOI_T",
        "Ti8XQCTWhgiA",
        "V8mNgOxNhgiB",
        "vGiQv5l4hgiB",
        "_PmsI_-55Y9p"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uCEQLS3zZUn"
      },
      "source": [
        "# Playing Starcraft and Saving the World Using Multi-agent Reinforcement Learning!\n",
        "\n",
        "<img src=\"https://static.starcraft2.com/dist/images/content/f2p-cards/img-f2p-campaign.jpg\" />\n",
        "\n",
        "Image Source:  https://starcraft2.com/en-gb/ .\n",
        "\n",
        "Powered By:\n",
        "<img src=\"https://raw.githubusercontent.com/instadeepai/Mava/develop/docs/images/mava.png\" />\n",
        "\n",
        "and [SMAC - StarCraft Multi-Agent Challenge](https://github.com/oxwhirl/smac).\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/instadeepai/IndabaX-SA-2021/blob/main/IndabaX_SA_Playing_Starcraft_and_Saving_the_World_Using_Multi_agent_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEAq7x7ff1fE"
      },
      "source": [
        "## 1. Installation\n",
        "**Installation of StarCraft II can take between 5 - 25 minutes.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUyjkLHP2jcj",
        "outputId": "212aae02-6a30-4492-f4e3-cd6d23ba4893"
      },
      "source": [
        "#@title Timing Tool (Run Cell)\n",
        "%%capture\n",
        "!pip install ipython-autotime\n",
        "\n",
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 2.07 ms (started: 2021-10-29 10:21:01 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl4ed6X22tZq",
        "cellView": "form"
      },
      "source": [
        "#@title Install Mava and Some Supported Environments (Run Cell)\n",
        "%%capture\n",
        "!pip install git+https://github.com/instadeepai/Mava@feature/smac-env-upgrades#egg=id-mava[reverb,tf,launchpad,envs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDYrT7BVw7Dx",
        "cellView": "form"
      },
      "source": [
        "#@title Installs and Imports for Agent Visualization (Run Cell)\n",
        "%%capture\n",
        "!pip install git+https://github.com/instadeepai/Mava#egg=id-mava[record_episode]\n",
        "! apt-get update -y &&  apt-get install -y xvfb &&  apt-get install -y python-opengl && apt-get install ffmpeg && apt-get install python-opengl -y && apt install xvfb -y && pip install pyvirtualdisplay \n",
        "\n",
        "import os\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(1280,720))\n",
        "display.start()\n",
        "os.environ[\"DISPLAY\"] = \":\" + str(display.display)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpDw1hTvrvXy",
        "cellView": "form"
      },
      "source": [
        "#@title Install Starcraft II (Run Cell)\n",
        "%%capture\n",
        "\n",
        "# Install Smac\n",
        "!pip install git+https://github.com/oxwhirl/smac.git\n",
        "\n",
        "# Install and Download Starcraft 2\n",
        "!wget https://raw.githubusercontent.com/instadeepai/Mava/develop/install_sc2.sh\n",
        "!chmod +x install_sc2.sh && ./install_sc2.sh\n",
        "\n",
        "# https://github.com/deepmind/pysc2/issues/327\n",
        "!apt-get remove libtcmalloc*\n",
        "\n",
        "import os\n",
        "os.environ['SC2PATH'] = \"/content/3rdparty/StarCraftII\"\n",
        "os.environ['LD_PRELOAD'] = ''\n",
        "\n",
        "# Let give us more time - increase episode limit of 3m from 60 - 120 (so we can train on colab)\n",
        "!sudo apt install vim\n",
        "!vim -c \"%s/: 60/: 180/g|wq\" /usr/local/lib/python3.7/dist-packages/smac/env/starcraft2/maps/smac_maps.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SGFGmWnhuI2"
      },
      "source": [
        "## 2. Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "8SvWrsWExz31"
      },
      "source": [
        "#@title Imports Modules (Run Cell)\n",
        "import functools\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, Mapping, Sequence, Union\n",
        "\n",
        "import launchpad as lp\n",
        "import numpy as np\n",
        "import sonnet as snt\n",
        "import tensorflow as tf\n",
        "from absl import app, flags\n",
        "from acme import types\n",
        "from mava.components.tf import networks\n",
        "from acme.tf import utils as tf2_utils\n",
        "\n",
        "\n",
        "from mava import specs as mava_specs\n",
        "from mava.systems.tf import vdn\n",
        "from mava.utils import lp_utils\n",
        "from mava.utils.environments import debugging_utils\n",
        "from mava.wrappers import MonitorParallelEnvironmentLoop\n",
        "from mava.components.tf import architectures\n",
        "from mava.utils.loggers import logger_utils\n",
        "from mava.components.tf.modules.exploration import LinearExplorationScheduler\n",
        "from mava.components.tf.modules.exploration import LinearExplorationTimestepScheduler\n",
        "\n",
        "# Seed random variables\n",
        "magic_random_seed=42\n",
        "\n",
        "import random\n",
        "import os\n",
        "def seed_data(seed: int) -> None:\n",
        "    print(f\"Using seed {seed}\")\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
        "    os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
        "\n",
        "seed_data(magic_random_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONjur9s1dK_1"
      },
      "source": [
        "## 3. SMAC: The StarCraft Multi-Agent Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qY77_tnR2fo"
      },
      "source": [
        "### SMAC Envrionment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "UPui24FMJ_DY"
      },
      "source": [
        "#@title SMAC: The StarCraft Multi-Agent Challenge Video (Run Cell)\n",
        "from IPython.display import YouTubeVideo\n",
        "# Video Source https://www.youtube.com/watch?v=VZ7zmQ_obZ0\n",
        "YouTubeVideo('VZ7zmQ_obZ0',width=1024, height=576)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK908RO8a0dH"
      },
      "source": [
        "SMAC is a popular environment/benchmark for cooperative MARL systems. \n",
        "\n",
        "It provides:\n",
        "- Partial Observability.\n",
        "- Challenging Dynamics.\n",
        "- High-dimensional observation spaces.\n",
        "\n",
        "Each unit is an **indepedent RL agent**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTODoG38ez4R"
      },
      "source": [
        "#### Observations, Actions and Rewards\n",
        "\n",
        "<img src=\"http://whirl.cs.ox.ac.uk/blog/wp-content/uploads/2019/01/smac_agent_obs-768x546.jpg\"  width=\"600\"  />\n",
        "\n",
        "**Observation**\n",
        "\n",
        "Local observations about both allied and enemy units which are within the sight range:\n",
        "- distance\n",
        "- relative x\n",
        "- relative y\n",
        "- health\n",
        "- shield\n",
        "- unit type\n",
        "- last action (only for allied units)\n",
        "\n",
        "**Actions** \n",
        "\n",
        "Agents can take the following discrete actions:\n",
        "- move[direction] (four directions: north, south, east, or west)\n",
        "- attack[enemy id]\n",
        "- heal[agent id] (only for Medivacs)\n",
        "- stop\n",
        "\n",
        "**Rewards**\n",
        "\n",
        "Reward signal calculated from the hit-point damage dealt and received by agents, some positive (negative) reward after having enemy (allied) units killed and/or a positive (negative) bonus for winning (losing) the battle - [calculation](https://github.com/oxwhirl/smac/blob/503a678c7378a08d5c55233cfa7ef967acfc93c5/smac/env/starcraft2/starcraft2.py#L824)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohA5m0REjhu-"
      },
      "source": [
        "### Select Map \n",
        "SMAC has 22 predifined combat scenarios ranging from **3 to 27** units. Full list of maps are available [here](https://github.com/oxwhirl/smac/blob/master/docs/smac.md)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Fw_4dR1jj-Wv"
      },
      "source": [
        "from mava.utils.environments import smac_utils\n",
        "\n",
        "map = '3m' #@param {type:\"string\"}\n",
        "\n",
        "\n",
        " # environment\n",
        "environment_factory = functools.partial(\n",
        "    smac_utils.make_environment, map_name=map, random_seed=magic_random_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcZKJhnyk45C"
      },
      "source": [
        "## 4. Train a `VDN` System to Play StarCraft II "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBWiibHIleQk"
      },
      "source": [
        "### Run Multi-Agent VDN System.\n",
        "\n",
        "[VDN](https://arxiv.org/abs/1706.05296)\n",
        "\n",
        "The Value Decomposition Network architecture (VDN) learns to **decompose\n",
        "the team value function into agent-wise value functions** from the team reward\n",
        "signal, by back-propagating the total Q gradient through deep neural networks\n",
        "representing the individual component value functions. VDN tries to alleviate\n",
        "the problem of unexplainable reward signals that emerge in purely independent\n",
        "learners. Moreover, since the value function learned by each agent depends only\n",
        "on local observations, it is more easily learned than the centralised joint value function. \n",
        "\n",
        "\n",
        "VDN fits into the popular multi-agent RL paradigm of centralised training with decentralised execution, i.e., agents learn in a centralised\n",
        "fashion at training time, but can be deployed individually.\n",
        "\n",
        "The main assumption made by VDN is that the joint action-value function for the system can\n",
        "be **additively** decomposed into value functions across agents,\n",
        "\n",
        "$$\n",
        "Q\\left(\\left(o^{1}, o^{2}, \\ldots, o^{n}\\right),\\left(a^{1}, a^{2}, \\ldots, a^{n}\\right)\\right) \\approx \\sum_{i=1}^{n} \\tilde{Q}_{i}\\left(o^{i}, a^{i}\\right), \n",
        "$$\n",
        "\n",
        "where $n$ is the number of agents, $o^i$ is the local observation of agent $i$ and $a^i$ is its action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avvSeVahk_Nt"
      },
      "source": [
        "#### Specify logging and checkpointing config. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "87bH5cn4LvhL"
      },
      "source": [
        "#@title Logging config. (Run Cell)\n",
        "# Directory to store checkpoints and log data. \n",
        "base_dir = \"/root/mava/\"\n",
        "\n",
        "# File name \n",
        "mava_id = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
        "\n",
        "# Log every [log_every] seconds\n",
        "log_every = 0.0\n",
        "logger_factory = functools.partial(\n",
        "    logger_utils.make_logger,\n",
        "    directory=base_dir,\n",
        "    to_terminal=True,\n",
        "    to_tensorboard=True,\n",
        "    time_stamp=mava_id,\n",
        "    time_delta=log_every,\n",
        ")\n",
        "\n",
        "# Checkpointer appends \"Checkpoints\" to checkpoint_dir\n",
        "checkpoint_dir = f\"{base_dir}/{mava_id}\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8XqA9M2iyK_"
      },
      "source": [
        "### Define Q Networks\n",
        "We will use the default Q networks for the `VDN` system.\n",
        "\n",
        "More details on the architecture [here](https://github.com/instadeepai/Mava/blob/develop/mava/systems/tf/vdn/networks.py) and [here](https://github.com/instadeepai/Mava/blob/develop/mava/systems/tf/madqn/networks.py). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ4-cN2dkXjq"
      },
      "source": [
        "network_factory = lp_utils.partial_kwargs(vdn.make_default_networks,seed=magic_random_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i3tj4h-lTm4"
      },
      "source": [
        "#### Create VDN System.\n",
        "\n",
        "Description of variables and system available [here](https://github.com/instadeepai/Mava/blob/feature/smac-env-upgrades/mava/systems/tf/vdn/system.py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_QZb8UVoHWG"
      },
      "source": [
        "system =  vdn.VDN(\n",
        "        environment_factory=environment_factory,\n",
        "        network_factory=network_factory,\n",
        "        logger_factory=logger_factory,\n",
        "        \n",
        "        # Number of parallel experience generating processes\n",
        "        num_executors=1,\n",
        "        \n",
        "        # Epsilon decay parameters\n",
        "        exploration_scheduler_fn=LinearExplorationTimestepScheduler,\n",
        "        epsilon_min=0.05,\n",
        "        epsilon_decay_steps=50000,\n",
        "        \n",
        "        # Optimizer for Q networks\n",
        "        optimizer=snt.optimizers.RMSProp(\n",
        "            learning_rate=0.0005, epsilon=0.00001, decay=0.99\n",
        "        ),\n",
        "        batch_size=512,\n",
        "\n",
        "        checkpoint_subpath=checkpoint_dir,\n",
        "        executor_variable_update_period=1,\n",
        "        target_update_period=100,\n",
        "        max_gradient_norm=10.0,\n",
        "        eval_loop_fn=MonitorParallelEnvironmentLoop,\n",
        "        eval_loop_fn_kwargs={\"path\": checkpoint_dir, \"record_every\": 50, \"figsize\":(720,1280)},\n",
        "        max_executor_steps=100000,\n",
        "        samples_per_insert=None\n",
        "    ).build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE_135VP0I0x",
        "cellView": "form"
      },
      "source": [
        "%%capture\n",
        "#@title Kill old runs. (Run Cell)\n",
        "\n",
        "!pkill -9 launchpad\n",
        "!pkill -9 3rdpary\n",
        "!pkill -9 Main_Thread\n",
        "!pkill -9 process_entry"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsoLWPTClnMt"
      },
      "source": [
        "# Ensure only trainer runs on gpu, while other processes run on cpu. \n",
        "local_resources = lp_utils.to_device(program_nodes=system.groups.keys(),nodes_on_gpu=[\"trainer\"])\n",
        "\n",
        "lp.launch(\n",
        "    system,\n",
        "    lp.LaunchType.LOCAL_MULTI_PROCESSING,\n",
        "    terminal=\"output_to_files\",\n",
        "    local_resources=local_resources,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN2KNO5V11E1"
      },
      "source": [
        "### Logs and Outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfI2fNFeltBm"
      },
      "source": [
        "#### View outputs from the evaluator process.\n",
        "*You might need to wait a few moments after launching the run.*\n",
        "The `CUDA_ERROR_NO_DEVICE` error is expected since the GPU is only used by the trainer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OchHHlv-dqv"
      },
      "source": [
        "!cat /tmp/launchpad_out/evaluator/0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHf3jDe3ySk7"
      },
      "source": [
        "#### View Stored Data \n",
        "*You might need to wait a few moments after launching the run.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPahKjTnqBAO"
      },
      "source": [
        "! ls $base_dir/$mava_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHygoBPW-3KV"
      },
      "source": [
        "### Tensorboard\n",
        "*You might need to wait a few moments after launching the run.*\n",
        "\n",
        "While waiting you can look at a previous [run](https://tensorboard.dev/experiment/osGaqNCEQl2tA6wNGmBlEw/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l181SBwtBo9M"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XNJyNsIDb_S"
      },
      "source": [
        "%tensorboard --logdir $base_dir/$mava_id/tensorboard/evaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDlUXGltyVhM"
      },
      "source": [
        "### View Agent Recording\n",
        "If the agents are trained, we should see emergence of behaviour like *focus fire*, *learned formations based on armour types* (not in 3m) and making enemy *units give chase* while maintaining enough distance so that little or no damage is done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpTFrSEpc0BU"
      },
      "source": [
        "#### Behaviour Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "EHHPtx1Va8aC"
      },
      "source": [
        "#@title Poorly trained agents - Agents running away. (RUN ME)\n",
        "import IPython\n",
        "IPython.display.HTML(url='https://raw.githubusercontent.com/instadeepai/Mava/feature/smac-env-upgrades/docs/images/runaway.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "hUYdburLcMA3"
      },
      "source": [
        "#@title Trained Agents - Emergence of focus fire. (RUN ME)\n",
        "import IPython\n",
        "IPython.display.HTML(url='https://raw.githubusercontent.com/instadeepai/Mava/feature/smac-env-upgrades/docs/images/focus_fire.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMA_Sm14dCF4"
      },
      "source": [
        "#### Our Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2l8o2zDBbuN"
      },
      "source": [
        "Check if any agent recordings are available. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXB1IKfysMT6"
      },
      "source": [
        "! ls $base_dir/$mava_id/recordings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjcnXbl7BfJc"
      },
      "source": [
        "#### View agent recordings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwrLcMufH_4q"
      },
      "source": [
        "import glob\n",
        "import os \n",
        "import IPython\n",
        "\n",
        "# Recordings\n",
        "list_of_files = glob.glob(f\"{base_dir}/{mava_id}/recordings/*.html\")\n",
        "\n",
        "if(len(list_of_files) == 0):\n",
        "  print(\"No recordings are available yet. Please wait or run the 'Run Multi-Agent VDN System.' cell if you haven't already done this.\")\n",
        "else:\n",
        "  latest_file = max(list_of_files, key=os.path.getctime)\n",
        "  first_file =  min(list_of_files, key=os.path.getctime)\n",
        "  print(\"Run the next cell to visualize your agents!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "95GOv5vc5z5Q"
      },
      "source": [
        "#@title Our agents early in training. (RUN ME)\n",
        "IPython.display.HTML(filename=first_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "svTddCh3IdH9"
      },
      "source": [
        "#@title Our most recent agent. (RUN ME)\n",
        "IPython.display.HTML(filename=latest_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYekMtHB26yL"
      },
      "source": [
        "## 5. What's next?\n",
        "- Scaling. \n",
        "- Run MARL System with larger q networks or different hyperparams.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TwjhQ0K4_yd"
      },
      "source": [
        "### Scaling\n",
        "Mava allows for simple scaling of MARL systems. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "mAe8wVIVNnIh"
      },
      "source": [
        "%%capture\n",
        "#@title Kill old runs. (Run Cell)\n",
        "\n",
        "!pkill -9 launchpad\n",
        "!pkill -9 3rdpary\n",
        "!pkill -9 Main_Thread\n",
        "!pkill -9 process_entry"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "hNJpEGUONnIi"
      },
      "source": [
        "#@title Logging config. (Run Cell)\n",
        "# Directory to store checkpoints and log data. \n",
        "base_dir = \"/root/mava/\"\n",
        "\n",
        "# File name \n",
        "mava_id = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
        "\n",
        "# Log every [log_every] seconds\n",
        "log_every = 0.0\n",
        "logger_factory = functools.partial(\n",
        "    logger_utils.make_logger,\n",
        "    directory=base_dir,\n",
        "    to_terminal=True,\n",
        "    to_tensorboard=True,\n",
        "    time_stamp=mava_id,\n",
        "    time_delta=log_every,\n",
        ")\n",
        "\n",
        "# Checkpointer appends \"Checkpoints\" to checkpoint_dir\n",
        "checkpoint_dir = f\"{base_dir}/{mava_id}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsyoRbRHNwdN"
      },
      "source": [
        "Simply increase the **num_executors**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hojXkt3FfKND"
      },
      "source": [
        "network_factory = lp_utils.partial_kwargs(vdn.make_default_networks,seed=magic_random_seed)\n",
        "\n",
        "system =  vdn.VDN(\n",
        "        environment_factory=environment_factory,\n",
        "        network_factory=network_factory,\n",
        "        logger_factory=logger_factory,\n",
        "        \n",
        "        # Number of parallel experience generating processes\n",
        "        num_executors=2,\n",
        "        \n",
        "        # Epsilon decay parameters\n",
        "        exploration_scheduler_fn=LinearExplorationTimestepScheduler,\n",
        "        epsilon_min=0.05,\n",
        "        epsilon_decay_steps=50000,\n",
        "        \n",
        "        # Optimizer for Q networks\n",
        "        optimizer=snt.optimizers.RMSProp(\n",
        "            learning_rate=0.0005, epsilon=0.00001, decay=0.99\n",
        "        ),\n",
        "        batch_size=512,\n",
        "\n",
        "        checkpoint_subpath=checkpoint_dir,\n",
        "        executor_variable_update_period=1,\n",
        "        target_update_period=100,\n",
        "        max_gradient_norm=10.0,\n",
        "        eval_loop_fn=MonitorParallelEnvironmentLoop,\n",
        "        eval_loop_fn_kwargs={\"path\": checkpoint_dir, \"record_every\": 50, \"figsize\":(720,1280)},\n",
        "        max_executor_steps=100000,\n",
        "        samples_per_insert=None\n",
        "    ).build()\n",
        "\n",
        "\n",
        "# Ensure only trainer runs on gpu, while other processes run on cpu. \n",
        "local_resources = lp_utils.to_device(program_nodes=system.groups.keys(),nodes_on_gpu=[\"trainer\"])\n",
        "\n",
        "lp.launch(\n",
        "    system,\n",
        "    lp.LaunchType.LOCAL_MULTI_PROCESSING,\n",
        "    terminal=\"output_to_files\",\n",
        "    local_resources=local_resources,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKne7VqnOI_O"
      },
      "source": [
        "##### View logs\n",
        "*You might need to wait a few moments after launching the run.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tsn_0AzOI_O"
      },
      "source": [
        "cat /tmp/launchpad_out/evaluator/0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmkPnQalOI_T"
      },
      "source": [
        "#### Tensorboard\n",
        "You might need to wait a few moments after launching the run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibXFev3nhdUK"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0ttIuwvOI_T"
      },
      "source": [
        "%tensorboard --logdir ~/mava/$mava_id/tensorboard/evaluator "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti8XQCTWhgiA"
      },
      "source": [
        "### Run MARL System with larger Q networks or different hyperparams.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "gRHm3gxBhgiA"
      },
      "source": [
        "%%capture\n",
        "#@title Kill old runs. (Run Cell)\n",
        "\n",
        "!pkill -9 launchpad\n",
        "!pkill -9 3rdpary\n",
        "!pkill -9 Main_Thread\n",
        "!pkill -9 process_entry"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "3caSo8_bhgiA"
      },
      "source": [
        "#@title Logging config. (Run Cell)\n",
        "# Directory to store checkpoints and log data. \n",
        "base_dir = \"/root/mava/\"\n",
        "\n",
        "# File name \n",
        "mava_id = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
        "\n",
        "# Log every [log_every] seconds\n",
        "log_every = 0.0\n",
        "logger_factory = functools.partial(\n",
        "    logger_utils.make_logger,\n",
        "    directory=base_dir,\n",
        "    to_terminal=True,\n",
        "    to_tensorboard=True,\n",
        "    time_stamp=mava_id,\n",
        "    time_delta=log_every,\n",
        ")\n",
        "\n",
        "# Checkpointer appends \"Checkpoints\" to checkpoint_dir\n",
        "checkpoint_dir = f\"{base_dir}/{mava_id}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7vG8p4shgiA"
      },
      "source": [
        "Change hyperparameters - larger Q network, different optimizer + LR decay."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY5Pl9XohgiB"
      },
      "source": [
        "network_factory = lp_utils.partial_kwargs(vdn.make_default_networks,seed=magic_random_seed,policy_networks_layer_sizes=(512, 512, 256))\n",
        "\n",
        "# LR that's lr for the first 4001 steps, lr * 0.1 for the next 6000 steps,\n",
        "# and lr * 0.01 for any additional steps.\n",
        "\n",
        "boundaries = [4000, 10000]\n",
        "lr = 0.01\n",
        "values = [lr, lr * 0.1, lr * 0.01]\n",
        "learning_rate_scheduler_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    boundaries, values\n",
        ")\n",
        "\n",
        "system =  vdn.VDN(\n",
        "        environment_factory=environment_factory,\n",
        "        network_factory=network_factory,\n",
        "        logger_factory=logger_factory,\n",
        "        \n",
        "        # Number of parallel experience generating processes\n",
        "        num_executors=1,\n",
        "        \n",
        "        # Epsilon decay parameters\n",
        "        exploration_scheduler_fn=LinearExplorationTimestepScheduler,\n",
        "        epsilon_min=0.05,\n",
        "        epsilon_decay_steps=50000,\n",
        "        \n",
        "        # Optimizer for Q networks\n",
        "        optimizer=snt.optimizers.SGD(learning_rate=lr),\n",
        "        learning_rate_scheduler_fn=learning_rate_scheduler_fn,\n",
        "        batch_size=512,\n",
        "\n",
        "        checkpoint_subpath=checkpoint_dir,\n",
        "        executor_variable_update_period=1,\n",
        "        target_update_period=100,\n",
        "        max_gradient_norm=10.0,\n",
        "        eval_loop_fn=MonitorParallelEnvironmentLoop,\n",
        "        eval_loop_fn_kwargs={\"path\": checkpoint_dir, \"record_every\": 50, \"figsize\":(720,1280)},\n",
        "        max_executor_steps=100000,\n",
        "        samples_per_insert=None\n",
        "    ).build()\n",
        "\n",
        "\n",
        "# Ensure only trainer runs on gpu, while other processes run on cpu. \n",
        "local_resources = lp_utils.to_device(program_nodes=system.groups.keys(),nodes_on_gpu=[\"trainer\"])\n",
        "\n",
        "lp.launch(\n",
        "    system,\n",
        "    lp.LaunchType.LOCAL_MULTI_PROCESSING,\n",
        "    terminal=\"output_to_files\",\n",
        "    local_resources=local_resources,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8mNgOxNhgiB"
      },
      "source": [
        "##### View logs\n",
        "*You might need to wait a few moments after launching the run.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVYyG_MShgiB"
      },
      "source": [
        "cat /tmp/launchpad_out/evaluator/0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGiQv5l4hgiB"
      },
      "source": [
        "#### Tensorboard\n",
        "You might need to wait a few moments after launching the run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E74VcV6HhgiB"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BsjTGsmhgiB"
      },
      "source": [
        "%tensorboard --logdir ~/mava/$mava_id/tensorboard/evaluator "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PmsI_-55Y9p"
      },
      "source": [
        "## For more examples using different systems, environments and architectures, visit our [github page](https://github.com/instadeepai/Mava/tree/develop/examples)."
      ]
    }
  ]
}